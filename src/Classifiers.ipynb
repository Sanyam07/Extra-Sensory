{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from General import *\n",
    "from ReadingTheDataUtils import *\n",
    "from Classifiers import *\n",
    "from PreproccesUtils import *\n",
    "from PerformanceEvalutionUtils import *\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT_PATH = Path('..')\n",
    "SRC_PATH = Path('.')\n",
    "DATA_PATH = ROOT_PATH / 'data'\n",
    "CSV_PATH = DATA_PATH / 'ExtraSensory.per_uuid_features_labels'\n",
    "CSV_SUFFIX = '.features_labels.csv'\n",
    "ORIGINAL_LABLES_CSV_PATH = DATA_PATH / 'ExtraSensory.per_uuid_original_labels'\n",
    "ORIGINAL_LABLES_CSV_SUFFIX = '.original_labels.csv'\n",
    "FOLD_PATH = DATA_PATH / 'cv_5_folds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = pd.read_csv(DATA_PATH / 'dataset.csv', index_col='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['label'].astype('category')\n",
    "\n",
    "for col in data.columns:\n",
    "    if col.startswith('discrete'):\n",
    "        data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn all classifiers by folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_list, test_folds_list = get_folds_list(FOLD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds_list, test_folds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_to_scale(i_X_fold_train):\n",
    "    cols_to_scale = i_X_fold_train.select_dtypes(exclude=['category']).columns\n",
    "    categorial_cols = i_X_fold_train.select_dtypes(include=['category']).columns\n",
    "    \n",
    "    return cols_to_scale, categorial_cols\n",
    "\n",
    "def get_X_data_dummies(i_X_fold_train, i_X_fold_test):\n",
    "    X_train = pd.get_dummies(i_X_fold_train, dummy_na=False).reset_index(drop=True, inplace=False)\n",
    "    X_test = pd.get_dummies(i_X_fold_test, dummy_na=False).reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def data_preproccessing(i_data, i_train_folds_list, i_test_folds_list):\n",
    "    # Get folds as DataFrame\n",
    "    train_fold_df, test_fold_df = get_folds_train_and_test(i_data, i_train_folds_list, i_test_folds_list)\n",
    "    X_fold_train, X_fold_test, y_fold_train, y_fold_test = \\\n",
    "            split_fold_data_to_features_and_labels(train_fold_df, test_fold_df)\n",
    "    \n",
    "    # Decide the scaling method, Numeric standard, categorial one-hot\n",
    "    cols_to_scale, categorial_cols = get_cols_to_scale(X_fold_train)\n",
    "    standard_X_train, standard_X_test = standard_data_scaling(X_fold_train[cols_to_scale], X_fold_test[cols_to_scale])\n",
    "    dummies_X_train, dummies_X_test = get_X_data_dummies(X_fold_train[categorial_cols], X_fold_test[categorial_cols])\n",
    "    \n",
    "    # Concat the numeric with the categorial\n",
    "    X_train = pd.concat([standard_X_train, dummies_X_train], axis=1)\n",
    "    X_test = pd.concat([standard_X_test, dummies_X_test], axis=1)\n",
    "    \n",
    "    return X_train, X_test, y_fold_train, y_fold_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# classifiers = dict()\n",
    "\n",
    "# for train_fold_lst, test_fold_lst in zip(train_folds_list, test_folds_list):\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # Preprocess the data\n",
    "#     standard_X_train, standard_X_test, y_fold_train, y_fold_test = \\\n",
    "#                 data_preproccessing(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "# #     handle_nulls_in_X(standard_X_train, standard_X_test)\n",
    "    \n",
    "#     # Learn all models sync\n",
    "# #     models_tuple = learn_all_models_sync(standard_X_train, y_fold_train)\n",
    "\n",
    "#     # Learn all model async\n",
    "#     models_tuple = learn_all_models_async(standard_X_train, y_fold_train, i_c_score_grid_search=True)\n",
    "    \n",
    "#     # Put each fold result in the classifiers dict\n",
    "#     classifiers.setdefault('single_sensor_classifier', []).append(models_tuple[0])\n",
    "#     classifiers.setdefault('early_fusion_classifier', []).append(models_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'classifiers_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the model to a file\n",
    "# with open(filename, 'wb') as outfile:\n",
    "#     pickle.dump(classifiers, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model from a file\n",
    "with open(filename, 'rb') as infile:\n",
    "    classifiers_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classifiers import NUM_OF_LABELS\n",
    "\n",
    "def get_class_weights_in_data(i_y):\n",
    "    y = np.array(i_y)\n",
    "    class_counts = np.unique(y, return_counts=True)[1]\n",
    "    \n",
    "    if len(class_counts) != NUM_OF_LABELS:\n",
    "        raise Exception(f\"class_counts length is diffrent from {NUM_OF_LABELS}\")\n",
    "        \n",
    "    return class_counts\n",
    "\n",
    "def get_test_weights(i_y_test):\n",
    "    return get_class_weights_in_data(i_y_test)\n",
    "    \n",
    "\n",
    "test_class_weights = np.zeros((NUM_OF_LABELS, ), dtype='int')\n",
    "\n",
    "\n",
    "for i, (train_fold_lst, test_fold_lst) in enumerate(zip(train_folds_list, test_folds_list)):\n",
    "    gc.collect()\n",
    "    \n",
    "    is_first_iteration = i == 0\n",
    "\n",
    "    \n",
    "    # Preprocess the data\n",
    "    standard_X_train, standard_X_test, y_fold_train, y_fold_test = \\\n",
    "                data_preproccessing(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "    test_class_weights = test_class_weights + get_test_weights(y_fold_test)\n",
    "    print(test_class_weights)\n",
    "    \n",
    "    if is_first_iteration:\n",
    "        single_sensors_states_dict, EF_states, LFA_states, LFL_states = get_states_arrays(data)\n",
    "    \n",
    "    # Get rows with all sensors data\n",
    "    feature_names = get_feature_names(standard_X_train, ['label'])  # In this case we use the data with our label!\n",
    "    sensor_names = get_sensor_names(feature_names)\n",
    "    \n",
    "    y_fold_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for _, sensor_cols_name_in_data in sensor_names.items():\n",
    "        mask = standard_X_test[sensor_cols_name_in_data].isnull().all(1)\n",
    "        idx_to_drop = standard_X_test[mask].index\n",
    "        \n",
    "        standard_X_test.drop(idx_to_drop, axis=0, inplace=True)\n",
    "        y_fold_test.drop(idx_to_drop, axis=0, inplace=True)\n",
    "\n",
    "    handle_nulls_in_X(standard_X_train, standard_X_test)\n",
    "    \n",
    "    single_sensor_models = classifiers_dict['single_sensor_classifier'][i]\n",
    "    \n",
    "    # Eeach single sensor model\n",
    "    get_single_sensor_state(single_sensors_states_dict, standard_X_test, y_fold_test, single_sensor_models)\n",
    "\n",
    "    # LFA\n",
    "    LFA_states = LFA_states + get_LFA_state(standard_X_test, y_fold_test,  single_sensor_models)\n",
    "        \n",
    "    # LFL\n",
    "    LFL_states = LFL_states + get_LFL_state(standard_X_train, y_fold_train, standard_X_test, y_fold_test, single_sensor_models)\n",
    "\n",
    "\n",
    "test_class_weights = (test_class_weights / test_class_weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFL_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evalutions_dict = dict()\n",
    "\n",
    "# Single sensor\n",
    "feature_names = get_feature_names(standard_X_test, ['label'])  # In this case we using the data with our label!\n",
    "sensor_names = get_sensor_names(feature_names)\n",
    "\n",
    "for sensor_name in sensor_names:\n",
    "    single_sesnor_state = single_sensors_states_dict[sensor_name]\n",
    "    TP, TN, FP, FN = single_sesnor_state[0].sum(), single_sesnor_state[1].sum(), single_sesnor_state[2].sum(), single_sesnor_state[3].sum()\n",
    "    sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        get_evaluations_metric_scores(TP, TN, FP, FN)\n",
    "    insert_values_to_evaluations_dict(evalutions_dict, sensor_name, \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFA\n",
    "TP, TN, FP, FN = LFA_states[0].sum(), LFA_states[1].sum(), LFA_states[2].sum(), LFA_states[3].sum()\n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        get_evaluations_metric_scores(TP, TN, FP, FN)                              \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFA\",\n",
    "                                  sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFL\n",
    "TP, TN, FP, FN = LFL_states[0].sum(), LFL_states[1].sum(), LFL_states[2].sum(), LFL_states[3].sum()\n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        get_evaluations_metric_scores(TP, TN, FP, FN) \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFL\", \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "pd.DataFrame.from_dict(evalutions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalutions_dict = dict()\n",
    "\n",
    "# Single sensor\n",
    "feature_names = get_feature_names(standard_X_test, ['label'])  # In this case we using the data with our label!\n",
    "sensor_names = get_sensor_names(feature_names)\n",
    "\n",
    "for sensor_name in sensor_names:\n",
    "    single_sesnor_state = single_sensors_states_dict[sensor_name]\n",
    "    scores_arr = np.zeros((6, ), dtype='int')\n",
    "\n",
    "    for c in range(NUM_OF_LABELS):\n",
    "        class_state = single_sesnor_state[:, c]\n",
    "        TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "        scores_arr = scores_arr + get_evaluations_metric_scores(TP, TN, FP, FN)\n",
    "\n",
    "    scores_arr = scores_arr / NUM_OF_LABELS\n",
    "    \n",
    "    sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "            scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "    insert_values_to_evaluations_dict(evalutions_dict, sensor_name, \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFA\n",
    "scores_arr = np.zeros((6, ), dtype='int')\n",
    "\n",
    "for c in range(NUM_OF_LABELS):\n",
    "    class_state = LFA_states[:, c]\n",
    "    TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "    scores_arr = scores_arr + get_evaluations_metric_scores(TP, TN, FP, FN)\n",
    "\n",
    "scores_arr = scores_arr / NUM_OF_LABELS\n",
    "\n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "    scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFA\",\n",
    "                                  sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFL\n",
    "scores_arr = np.zeros((6, ), dtype='int')\n",
    "\n",
    "for c in range(NUM_OF_LABELS):\n",
    "    class_state = LFL_states[:, c]\n",
    "    TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "    scores_arr = scores_arr + get_evaluations_metric_scores(TP, TN, FP, FN)\n",
    "\n",
    "scores_arr = scores_arr / NUM_OF_LABELS\n",
    "\n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFL\", \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "pd.DataFrame.from_dict(evalutions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalutions_dict = dict()\n",
    "\n",
    "# Single sensor\n",
    "feature_names = get_feature_names(standard_X_test, ['label'])  # In this case we using the data with our label!\n",
    "sensor_names = get_sensor_names(feature_names)\n",
    "\n",
    "for sensor_name in sensor_names:\n",
    "    single_sesnor_state = single_sensors_states_dict[sensor_name]\n",
    "    scores_arr = []\n",
    "\n",
    "    for c in range(NUM_OF_LABELS):\n",
    "        class_state = single_sesnor_state[:, c]\n",
    "        TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "        \n",
    "        scores_arr.append(get_evaluations_metric_scores(TP, TN, FP, FN))\n",
    "\n",
    "    scores_arr = np.dot(test_class_weights, np.array(scores_arr))\n",
    "\n",
    "    sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "            scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "    insert_values_to_evaluations_dict(evalutions_dict, sensor_name, \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFA\n",
    "scores_arr = []\n",
    "\n",
    "for c in range(NUM_OF_LABELS):\n",
    "    class_state = LFA_states[:, c]\n",
    "    TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "    \n",
    "    scores_arr.append(get_evaluations_metric_scores(TP, TN, FP, FN))\n",
    "\n",
    "scores_arr = np.dot(test_class_weights, np.array(scores_arr))\n",
    "    \n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFA\",\n",
    "                                  sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "# LFL\n",
    "scores_arr = []\n",
    "\n",
    "for c in range(NUM_OF_LABELS):\n",
    "    class_state = LFL_states[:, c]\n",
    "    TP, TN, FP, FN = class_state[0], class_state[1], class_state[2], class_state[3]\n",
    "    \n",
    "    scores_arr.append(get_evaluations_metric_scores(TP, TN, FP, FN))\n",
    "\n",
    "scores_arr = np.dot(test_class_weights, np.array(scores_arr))\n",
    "\n",
    "sensitivity, specifisity, accuracy, precision, BA, F1 =\\\n",
    "        scores_arr[0], scores_arr[1], scores_arr[2], scores_arr[3], scores_arr[4], scores_arr[5]                       \n",
    "insert_values_to_evaluations_dict(evalutions_dict, \"LFL\", \n",
    "                                 sensitivity, specifisity, accuracy, precision, BA, F1)\n",
    "\n",
    "pd.DataFrame.from_dict(evalutions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EF training using the pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_pre_pipe(X):\n",
    "    cat_cols = X.select_dtypes(include=[\"category\"]).columns\n",
    "    cat_colds_indices = [X.columns.get_loc(c) for c in cat_cols if c in X_fold_train]\n",
    "    num_cols = X.select_dtypes(exclude=[\"category\"]).columns\n",
    "    num_cols_indices = [X.columns.get_loc(c) for c in num_cols if c in X_fold_train]\n",
    "    \n",
    "    column_transformer_sacler = ColumnTransformer(\n",
    "        [\n",
    "            ('ohe', OneHotEncoder(sparse=False), cat_colds_indices),\n",
    "            ('scaler', StandardScaler(), num_cols_indices)\n",
    "        ]\n",
    "    )\n",
    "    column_transformer_null_handler = ColumnTransformer(\n",
    "        [\n",
    "            ('SimpleImputerCat', SimpleImputer(strategy=\"most_frequent\"), cat_colds_indices),\n",
    "            ('SimpleImputerNum', SimpleImputer(strategy='mean'), num_cols_indices)\n",
    "        ]\n",
    "    )\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            ('column_transformer_sacler', column_transformer_sacler),\n",
    "            ('column_transformer_null_handler', column_transformer_null_handler)\n",
    "        ], verbose=True\n",
    "    )\n",
    "        \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_models = {\n",
    "    'pipe': [],\n",
    "    'lr': [],\n",
    "    'rf': [],\n",
    "    'gnb': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each fold learn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import check_X_y\n",
    "\n",
    "\n",
    "for train_fold_lst, test_fold_lst in zip(train_folds_list, test_folds_list):\n",
    "    gc.collect()\n",
    "\n",
    "    train_fold_df, test_fold_df = get_folds_train_and_test(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "    X_fold_train, _, y_fold_train, _ = \\\n",
    "            split_fold_data_to_features_and_labels(train_fold_df, test_fold_df)\n",
    "    \n",
    "    pre_pipe = get_single_pre_pipe(X_fold_train)\n",
    "    X = pre_pipe.fit_transform(X_fold_train)\n",
    "    X, y = check_X_y(X, y_fold_train)\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    rf_param_grid = {'max_depth': [8, 16, 32],\n",
    "                     'class_weight': ['balanced'],\n",
    "                     'n_estimators': [8, 16, 32]}\n",
    "    rf_grid = GridSearchCV(RandomForestClassifier(),\n",
    "                           param_grid=rf_param_grid,\n",
    "                           verbose=3,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='f1_weighted',\n",
    "                           cv=3)\n",
    "#     rf_grid = RandomForestClassifier(n_jobs=-1, verbose=3)\n",
    "#     rf_grid.fit(X, y)\n",
    "\n",
    "#     # Linear Regression\n",
    "#     lr_param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "#                      'penalty' : ['l1', 'l2'],\n",
    "#                      'max_iter': [100],\n",
    "#                      'class_weight': ['balanced']}\n",
    "#     lr_grid = GridSearchCV(LogisticRegression(), param_grid=lr_param_grid, verbose=5, n_jobs=1, scoring='f1_weighted', cv=None)\n",
    "#     lr_grid = LogisticRegression(n_jobs=-1, verbose=3)\n",
    "#     lr_grid.fit(X, y)\n",
    "    \n",
    "#     # Gaussian Naive Bayes\n",
    "#     gnb_param_grid = {}\n",
    "#     gnb_grid = GridSearchCV(GaussianNB(), param_grid=gnb_param_grid, verbose=5, n_jobs=1, scoring='f1_weighted', cv=None)\n",
    "    gnb_grid = GaussianNB()\n",
    "    gnb_grid.fit(X, y)\n",
    "    \n",
    "    ef_models['pipe'].append(pre_pipe)\n",
    "#     ef_models['lr'].append(lr_grid)\n",
    "#     ef_models['rf'].append(rf_grid)\n",
    "    ef_models['gnb'].append(gnb_grid)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_models['gnb'][0].class_count_\n",
    "ef_models['gnb'][0].class_prior_\n",
    "ef_models['gnb'][0].classes_\n",
    "ef_models['gnb'][0].epsilon_\n",
    "ef_models['gnb'][0].sigma_\n",
    "ef_models['gnb'][0].theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid in ef_models['rf']:\n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each fold get model scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "model_f1_scores = {\n",
    "    'lr': [],\n",
    "    'rf': [],\n",
    "    'gnb': []\n",
    "}\n",
    "\n",
    "model_accuracy_score = {\n",
    "    'lr': [],\n",
    "    'rf': [],\n",
    "    'gnb': []\n",
    "}\n",
    "\n",
    "for idx, (train_fold_lst, test_fold_lst) in enumerate(zip(train_folds_list, test_folds_list)):\n",
    "    gc.collect()\n",
    "\n",
    "    # Split to X, y\n",
    "    train_fold_df, test_fold_df = get_folds_train_and_test(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "    X_fold_train, _, y_fold_train, _ = \\\n",
    "            split_fold_data_to_features_and_labels(train_fold_df, test_fold_df)\n",
    "    \n",
    "    X = ef_models['pipe'][idx].transform(X_fold_train)\n",
    "    X, y = check_X_y(X, y_fold_train)\n",
    "    \n",
    "    # predict\n",
    "#     lr_pred = ef_models['lr'][idx].predict(X)\n",
    "    rf_pred = ef_models['rf'][idx].predict(X)\n",
    "#     gnb_pred = ef_models['gnb'][idx].predict(X)\n",
    "    \n",
    "    # Save\n",
    "#     model_f1_scores['lr'].append(f1_score(y, lr_pred, average='weighted', sample_weight=y))\n",
    "    model_f1_scores['rf'].append(f1_score(y, rf_pred, average='weighted', sample_weight=y))\n",
    "#     model_f1_scores['gnb'].append(f1_score(y, gnb_pred, average='weighted', sample_weight=y))\n",
    "    \n",
    "#     model_accuracy_score['lr'].append(f1_score(y, lr_pred, average='weighted', sample_weight=y))\n",
    "    model_accuracy_score['rf'].append(f1_score(y, rf_pred, average='weighted', sample_weight=y))\n",
    "#     model_accuracy_score['gnb'].append(f1_score(y, gnb_pred, average='weighted', sample_weight=y))\n",
    "    \n",
    "\n",
    "num_of_folds = (idx + 1)\n",
    "for model_name, model_scores in model_f1_scores.items():\n",
    "    model_scores_arr = np.array(model_scores)\n",
    "    model_score_mean = model_scores_arr.sum() / num_of_folds\n",
    "    \n",
    "    print(f'{model_name}: {model_score_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "model_f1_scores = {\n",
    "    'lr': [],\n",
    "    'rf': [],\n",
    "    'gnb': []\n",
    "}\n",
    "\n",
    "model_accuracy_score = {\n",
    "    'lr': [],\n",
    "    'rf': [],\n",
    "    'gnb': []\n",
    "}\n",
    "\n",
    "for idx, (train_fold_lst, test_fold_lst) in enumerate(zip(train_folds_list, test_folds_list)):\n",
    "    gc.collect()\n",
    "\n",
    "    # split to X, y\n",
    "    train_fold_df, test_fold_df = get_folds_train_and_test(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "    _, X_fold_test, _, y_fold_test = \\\n",
    "            split_fold_data_to_features_and_labels(train_fold_df, test_fold_df)\n",
    "    \n",
    "    X = ef_models['pipe'][idx].transform(X_fold_test)\n",
    "    X, y = check_X_y(X, y_fold_test)\n",
    "    \n",
    "    # predict\n",
    "#     lr_pred = ef_models['lr'][idx].predict(X)\n",
    "    rf_pred = ef_models['rf'][idx].predict(X)\n",
    "#     gnb_pred = ef_models['gnb'][idx].predict(X)\n",
    "    \n",
    "    # Save\n",
    "#     model_f1_scores['lr'].append(f1_score(y, lr_pred, average='weighted', sample_weight=y))\n",
    "    model_f1_scores['rf'].append(f1_score(y, rf_pred, average='weighted', sample_weight=y))\n",
    "#     model_f1_scores['gnb'].append(f1_score(y, gnb_pred, average='weighted', sample_weight=y))\n",
    "    \n",
    "#     model_accuracy_score['lr'].append(f1_score(y, lr_pred, average='weighted', sample_weight=y))\n",
    "    model_accuracy_score['rf'].append(f1_score(y, rf_pred, average='weighted', sample_weight=y))\n",
    "#     model_accuracy_score['gnb'].append(f1_score(y, gnb_pred, average='weighted', sample_weight=y))\n",
    "\n",
    "    \n",
    "num_of_folds = (idx + 1)\n",
    "\n",
    "for model_name, model_scores in model_f1_scores.items():\n",
    "    model_scores_arr = np.array(model_scores)\n",
    "    model_score_mean = model_scores_arr.sum() / num_of_folds\n",
    "    \n",
    "    print(f'{model_name}: {model_score_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mean of each label % in the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_s = 0\n",
    "lr_s = 0\n",
    "rf_s = 0\n",
    "gnb_s = 0\n",
    "\n",
    "for idx, (train_fold_lst, test_fold_lst) in enumerate(zip(train_folds_list, test_folds_list)):\n",
    "    gc.collect()\n",
    "\n",
    "    train_fold_df, test_fold_df = get_folds_train_and_test(data, train_fold_lst, test_fold_lst)\n",
    "    \n",
    "    _, X_fold_test, _, y_fold_test = \\\n",
    "            split_fold_data_to_features_and_labels(train_fold_df, test_fold_df)\n",
    "    \n",
    "    # TODO: send the right params! \n",
    "    X = ef_models['pipe'][idx].transform(X_fold_test)\n",
    "    X, y = check_X_y(X, y_fold_test)\n",
    "    \n",
    "    # pred\n",
    "    lr_pred = ef_models['lr'][idx].predict(X)\n",
    "    rf_pred = ef_models['rf'][idx].predict(X)\n",
    "    gnb_pred = ef_models['gnb'][idx].predict(X)\n",
    "    \n",
    "    # count\n",
    "    _, y_counts = np.unique(y, return_counts=True)\n",
    "    y_s += y_counts / y_counts.sum()\n",
    "    \n",
    "    _, lr_counts = np.unique(lr_pred, return_counts=True)\n",
    "    lr_s += lr_counts / lr_counts.sum()\n",
    "    \n",
    "    _, rf_counts = np.unique(rf_pred, return_counts=True)\n",
    "    rf_s += rf_counts / rf_counts.sum()\n",
    "    \n",
    "    _, gnb_counts = np.unique(gnb_pred, return_counts=True)\n",
    "    gnb_s += gnb_counts / gnb_counts.sum()\n",
    "    \n",
    "    \n",
    "y_s /= 5\n",
    "lr_s /= 5\n",
    "rf_s /= 5\n",
    "gnb_s /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "X = np.arange(y_s.shape[0])\n",
    "\n",
    "ax.bar(X - 0.20, lr_s, width=0.20)\n",
    "ax.bar(X + 0.00, rf_s, width=0.20)\n",
    "ax.bar(X + 0.20, gnb_s, width=0.20)\n",
    "ax.bar(X + 0.40, y_s, width=0.20)\n",
    "\n",
    "ax.legend(['lr', 'rf', 'gnb', 'y'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
